# ---- Web framework (both GPU & CPU) ----
fastapi~=0.115
uvicorn[standard]~=0.30
pydantic~=2.9
httpx~=0.27
safetensors~=0.4

# ---- Core ML stack ----
# GPU / Kaggle (CUDA 12.1 wheels)
# If you're on Kaggle or a CUDA 12.1 machine, keep these two lines:
torch==2.3.1 --index-url https://download.pytorch.org/whl/cu121
# (Transformers will pull tokenizers; pin to a stable line)
transformers~=4.44
accelerate~=0.34
sentencepiece~=0.2
bitsandbytes~=0.43  # Linux-only; OK on Kaggle

# CPU-only alternative:
# torch==2.3.1
# transformers~=4.44
# accelerate~=0.34
# sentencepiece~=0.2
# (comment out bitsandbytes on Windows/CPU)

# ---- Optional: quantization / speed-ups (enable later) ----
# auto-gptq~=0.7           # for GPTQ-quantized weights (Linux)
# optimum~=1.22            # ONNX / BetterTransformer paths
# peft~=0.11               # LoRA adapters (Week 2+)

# ---- Dev & tests (optional) ----
# pytest~=8.3
# black~=24.8
# ruff~=0.6
